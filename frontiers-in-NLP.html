<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>UBC NLP Group | Frontiers in NLP</title>
  <meta name="description" content="The Natural Language Processing (NLP) group at University of British Columbia conducts research on core NLP problems, computational linguistics, text mining, and visual text analytics under Profs. Giuseppe Carenini, Raymond Ng and Vered Shwartz.">
  <link href="https://fonts.googleapis.com/css?family=Spartan:100,200,300,400,500,600,700,800,900&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto:100,100i,300,300i,400,400i,500,500i,700,700i,900,900i&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/materialize.css">
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  <link rel="stylesheet" href="/assets/css/style.css">
  <script src="/assets/js/materialize.min.js"></script>
  <script src="/assets/js/jquery.js"></script>
  <script src="/assets/js/custom.js"></script>
  <link rel="shortcut icon" type="image/png" href="/assets/img/nlp.png">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-361W0CW3JN"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-361W0CW3JN');
  </script>
</head>

  <body>
    <div class="page-content">
      <div class="row navbar-fixed z-depth-4" style="background-color: #97D4E9; margin-bottom: 0px; height: 90px;">
  <nav class="col l8 m10 s12 center-align offset-l2 offset-m1 z-depth-0" style="background-color: #97D4E9; height: 90px; padding:0px;" >
    <div class="col l12 m12 s12" style="height: 90px; padding:0px;">
      <div class="nav-wrapper" style="line-height:1.2;">
      <div class="col l3 m8 s8 left-align z-depth-0" >
        <a href="https://www.ubc.ca/">
          <img src="/assets/img/ubc.png" style="height:80%; max-height: 80px; max-width:120%; padding-top:15px; padding-left:15px; float: left; overflow: hidden;">
        </a>
      </div>
      <div style="padding-top:15px; float:right;">
        <a href="#" data-target="mobile" class="sidenav-trigger"><i class="large material-icons" style="color: #002145; font-size: 30px">menu</i></a>
      </div>
      <div class="right hide-on-med-and-down" style="padding-bottom: 0px; padding-top:30px; padding-right:10px; float:right;">
        <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/"><b>Home</b></a></span>
        
        
      
        
        
        
      
        
      
        
        
        
      
        
        
        
      
        
        
        
      
        
        
        
      
        
        
          <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/people"><b>People</b></a></span> 
        
        
      
        
        
          <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/publications"><b>Publications</b></a></span> 
        
        
      
        
        
          <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/reading-group"><b>Reading Group</b></a></span> 
        
        
      
        
        
          <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/showcase"><b>Showcase</b></a></span> 
        
        
      
        
        
          <span class="z-depth-1 waves-effect waves" style="padding: 4px;"><a style="color: #002145; font-size:10pt; font-weight:300" class="navbar-anchor" href="/social"><b>Socials</b></a></span> 
        
        
      
        
      
    </div>
    </div>
    </div>
  <div class="col l12 m12 s12 left-align z-depth-0 valign-wrapper" style="background-color: #002145; height: 80px; width: 100%">
    <a href="/"><img src="/assets/img/nlp.png" style="height:60px; padding-left:10px; float: right;"></a>
    <p style="line-height:1.5; margin-left: 20px">
        <span><a href="https://www.cs.ubc.ca/">Computer Science</a> and <a href="https://linguistics.ubc.ca/">Linguistics</a></span><br />
      <span style="font-size:14pt"><a href="/">Natural Language Processing Group</a></span>
    </p>
    </div>
  </nav>
</div>
<ul class="sidenav" id="mobile">
  <div style="padding: 10px; margin-top: 10px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/"><b>Home</b></a></div>
    
    
  
    
    
    
  
    
  
    
    
    
  
    
    
    
  
    
    
    
  
    
    
    
  
    
    
      <div style="padding: 10px; margin-top: 4px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/people"><b>People</b></a></div>
    
    
  
    
    
      <div style="padding: 10px; margin-top: 4px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/publications"><b>Publications</b></a></div>
    
    
  
    
    
      <div style="padding: 10px; margin-top: 4px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/reading-group"><b>Reading Group</b></a></div>
    
    
  
    
    
      <div style="padding: 10px; margin-top: 4px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/showcase"><b>Showcase</b></a></div>
    
    
  
    
    
      <div style="padding: 10px; margin-top: 4px"><a style="color: #002145; font-size:12pt; font-weight:300" class="navbar-anchor" href="/social"><b>Socials</b></a></div>
    
    
  
    
  
</ul>

<div class="row" style="padding-top: 80px;">

  	<div class="col l8 m10 s12 center-align offset-l2 offset-m1 z-depth-4" style="background-color: #FFFFFF; padding:0px; padding-bottom: 20px">
		<div style='margin-bottom: 10px; border-radius: 10px;'>
				
			    <div style='font-weight:300; text-align: left; padding-left: 10px; padding-right: 10px'>
			      	<h4 id="frontiers-in-nlp">Frontiers in NLP</h4>

<blockquote>
  <p><span style="font-size:1.6em;">Thursday, July 17, 12:30 - 17:40 </span> <br />
<span style="font-size:1.6em;"><a href="">UBC Computer Science Department-ICICS X836</a></span> <br /></p>
</blockquote>

<div style="text-align: center; margin-top: 20px;">
  <a href="https://docs.google.com/forms/d/e/1FAIpQLScp2r-RSRdcw1T862ArSZIcJHyo-hEsgCE9H5LWwHAQX-DOvg/viewform?usp=dialog" style="display: inline-block; padding: 10px 20px; font-size: 16px; color: white; background-color: #007BFF; border-radius: 5px; text-decoration: none;">Register Now</a>
</div>

<h5> Invited Speakers </h5>

<div style="display: flex; flex-direction: column; align-items: center; gap: 0px;">
<div style="text-align: center; margin: 10px;">
    <img src="/assets/img/frontiers-in-nlp/YuvalPinter.jpeg" alt="Yual Pinter" style="width: 150px; height: 150px; object-fit: cover; border-radius: 50%;" />
    <p><strong>Yuval Pinter</strong><br />Ben Gurion University<br /></p>
    <details style="text-align:left">
      <summary><strong>Talk Title:</strong> Building Character: Tokenization in Theory and in Practice </summary>
      <p><strong>Abstract:</strong> The end-to-end nature of recent NLP models, most prominently large language models (LLMs), makes it hard for us to figure out how their individual processing steps are affected by properties of text and language, for example how they represent individual words in alignment with how humans perceive them and build utterances from the bottom up. We can find instances where models struggle with lexical changes in register, domain, or innovation, but the underlying mechanisms still mostly elude us. &lt;/br&gt;In this talk, I will focus on the lexical and sub-lexical levels of LLM representations, challenging models with recognition of words and the processes they are formed through, looking at subword tokenization algorithms from the point of view of their downstream performance requirements and the challenges posed by different languages. I will present SaGe, a subword tokenizer that incorporates context into the vocabulary creation objective; BoundlessBPE, which allows breaking past the limits of regex pretokenization; and Splinter, a pre-tokenization algorithm that relinearizes Hebrew text to conform the concatenative tokenization pipeline with Semitic templatic morphology.</p>
      <p><strong>Bio:</strong> Yuval Pinter is a Senior Lecturer in the Department of Computer Science at Ben-Gurion University of the Negev, focusing on NLP as PI of the MeLeL lab. Yuval got his PhD at the Georgia Institute of Technology School of Interactive Computing as a Bloomberg Data Science PhD Fellow. Prior to this, he worked as a Research Engineer at Yahoo Labs and as a Computational Linguist at Ginger Software, and obtained an MA in Linguistics and a BSc in CS and Mathematics, both from Tel Aviv University.</p>
    </details>
  </div><br />

  <div style="text-align: center; margin: 10px;">
    <img src="/assets/img/frontiers-in-nlp/Tuhin-Chakrabarty.png" alt="Tuhin Chakrabarty" style="width: 150px; height: 150px; object-fit: cover; border-radius: 50%;" />
    <p><strong>Tuhin Chakrabarty</strong><br />Stony Brook University</p>
    <details style="text-align:left">
      <summary><strong>Talk Title:</strong> GPTs are Un‘fair’ : An Early Look at the Labor Market Dilution Potential by Training on Copyrighted Data </summary>
      <p><strong>Abstract:</strong> Prior work on Generative AI and Copyright has primarily focused on memorization and verbatim regurgitation of copyrighted data that may or may not violate Fair Use. The fourth fair use factor instructs courts to inquire into the effect of the use upon the potential market for or value of the copyrighted work. While there's been compelling arguments in Copyright cases alongside few anecdotal evidences on how training AI on copyrighted books poses risk towards potential dilution of labor market, there has been no large scale data driven empirical research to support such claims.  To understand impact of AI on the Future of Creative Labor we conducted a large scale behavioral experiment where MFA candidates from top writing programs in the US were pitted against state of the art large language models. MFA candidates were compensated to reproduce an excerpt written by an award winning author using the authors distinctive style and voice based on a provided content. LLMs were then assigned to perform the same task under two treatment conditions— i) using the same long context, few shot prompts provided to MFAs ii) by fine-tuning on authors entire oeuvre. Through planned pairwise contrasts and blind assessments between treatment conditions we evaluated the resulting excerpts by both experts and lay readers recruited from Prolific who rate each pair on (i) overall writing quality and (ii) stylistic fidelity to the original author without knowing if a given text was human written or AI-generated. Our results reveal striking differences between AI approaches across lay readers and experts. Lay readers consistently preferred AI-generated excerpts over MFA-written ones regardless of the treatment condition. In contrast based on expert judgements, while in-context prompting produced excerpts were less favored than those written by MFA candidates,  excerpts generated by fine-tuned models surpassed MFAs. These findings empirically validate several commenters' assertions that fine-tuning requires distinct considerations under the first fair use factor. Equally concerning, analysis using Pangram (a state of art AI detection tool) revealed stark detectability contrasts. Few-shot AI-generated text was identified with 100% accuracy, while fine-tuned AI outputs achieved an alarmingly low 0% detection rate.  Fine-tuned models produce undetectable, high-quality outputs competing directly with human creators, potentially devaluing original work and undermining economic incentives copyright law protects. Additionally preference of AI generated text among lay readers is particularly significant as they represent the actual consumer. These results demonstrate compelling early evidence that unauthorized use of copyrighted data poses genuine threats to creative labor markets.</p>
      <p><strong>Bio:</strong> Tuhin Chakrabarty is  Assistant Professor at the Computer Science Department in Stony Brook University (SUNY). Prior to this he obtained his PhD from Columbia University where his research supported by an Amazon Fellowship and a NYTimes R&amp;D fellowship.His research interests are broadly in AI, NLP and Human AI Interaction and his goal is to design and build reliable AI systems that can handle implicature and ambiguity, understand human behavior and are aligned with the requirements humans have from technology. His research often relies on knowledge, methods, and perspectives from multiple disciplines to address complex problems or questions that cannot be fully understood or solved within the boundaries of Computer Science. Tuhin's work has been covered in MIT Tech Review, Bloomberg, Washington Post and he has been the receipent of Best Paper Honorable Mention award at ACM CHI and more recently an Outstanding Position Paper award at ICML for his focus on building  prioritizing Human Centered AI.</p>
    </details>
  </div><br />
  
  <div style="text-align: center; margin: 10px;">
    <img src="/assets/img/frontiers-in-nlp/sarahwiegreffe.jpeg" alt="Sarah Wiegreffe" style="width: 150px; height: 150px; object-fit: cover; border-radius: 50%;" />
    <p><strong>Sarah Wiegreffe</strong><br />Allen Institute for AI</p>
    <details style="text-align:left">
      <summary><strong>Talk Title:</strong> Demystifying the Inner Workings of Language Models </summary>
      <p><strong>Abstract:</strong> Large language models (LLMs) power a rapidly-growing and increasingly impactful suite of AI technologies. However, due to their scale and complexity, we lack a fundamental scientific understanding of much of LLMs’ behavior, even when they are open source. The “black-box” nature of LMs not only complicates model debugging and evaluation, but also limits trust and usability. In this talk, I will describe how my research on interpretability (i.e., understanding models’ inner workings) has answered key scientific questions about how models operate. I will then demonstrate how deeper insights into LLMs’ behavior enable both 1) targeted performance improvements and 2) the production of transparent, trustworthy explanations for human users.</p>
      <p><strong>Bio:</strong> Sarah Wiegreffe is a current postdoctoral researcher at the Allen Institute for AI (Ai2) and the University of Washington, and an incoming (starting in August) assistant professor of Computer Science at the University of Maryland. She has worked on the explainability and interpretability of neural networks for NLP since 2017, with a focus on understanding how language models make predictions in order to make them more transparent to human users. She has been honored as a 3-time Rising Star in EECS (2023), Machine Learning (2024), and Generative AI (2024). She received her PhD in Computer Science from Georgia Tech in 2022, during which time she interned at Google and Ai2 and won the Ai2 outstanding intern award. She frequently serves on conference program committees, receiving outstanding area chair awards at ACL 2023 and EMNLP 2024. Her website is https://sarahwie.github.io/.</p>
    </details>
    
  </div><br />
  
  <div style="text-align: center; margin: 10px;">
    <img src="/assets/img/frontiers-in-nlp/Yossi_Levi.jpg" alt="Yossi Levi" style="width: 150px; height: 150px; object-fit: cover; border-radius: 50%;" />
    <p><strong>Yossi Levi</strong><br />Technion</p>
    <details style="text-align:left">
      <summary><strong>Talk Title:</strong> CLIP Latent space geometry </summary>
      <br /><strong>Abstract:</strong> The talk covers two papers to be presented at ICML 2025. I will present our recent work analyzing the geometry of CLIP’s latent space from both geometric and probabilistic perspectives. We show that the embedding space is better characterized by two distinct, shifted ellipsoids, rather than a shared hypersphere. This finding challenges common assumptions about CLIP’s latent structure.&lt;/br&gt;Building on this double-ellipsoid perspective, we introduce a new measure called conformity, which captures how closely a sample aligns with its modality mean. We observe that common concepts tend to align closely with the modality mean, while rare ones are significantly offset, suggesting a geometric analogue for the notion of concept commonality.&lt;/br&gt;Finally, I will introduce Whitened CLIP (W-CLIP) — a simple, linear transformation of the latent space into an isotropic space. It enables the use of embedding norms as a surrogate for likelihood approximation. This approach supports a wide range of applications, including domain shift detection and the identification of generative artifacts.&lt;/p&gt;
      <p><strong>Bio:</strong> Meir Yossef Levi (Yossi Levi) is in the final stages of his Ph.D. at the Technion, advised by Prof. Guy Gilboa, after receiving both his B.Sc. and M.Sc. in Electrical Engineering from the Technion. His research focuses on multimodal representation learning, with a particular interest in understanding the latent geometry of vision-language models and its implications. His recent work centers on CLIP, with two papers accepted to ICML 2025 on this topic. Prior to this, he studied robust classification in 3D vision, with publications at ICCV and 3DV.</p>
    </details>
  </div><br />
  
  <div style="text-align: center; margin: 10px;">
    <img src="/assets/img/frontiers-in-nlp/aaron-mueller.jpg" alt="Aaron Mueller" style="width: 150px; height: 150px; object-fit: cover; border-radius: 50%;" />
    <p><strong>Aaron Mueller</strong><br />Boston University</p>
    <details style="text-align:left">
      <summary><strong>Talk Title:</strong> How Do Language Models Think? A Causal Framework for Understanding Language Model Representations</summary>
      <p><strong>Abstract:</strong> The goal of language model (LM) interpretability is to explain the behaviors of LMs in terms of their internal computations. Achieving this could enable us to predict LM behaviors on future examples, or to precisely steer or edit their behaviors.  However, current interpretability methods lack principled foundations; there is little consensus on what the most effective scientific abstractions are, or how to evaluate progress. This talk is guided by two questions: (i) What are the right terms and abstractions for describing the inner workings of a language model? (ii) How would we know when we’ve found the right abstraction? I will first present a causal framework for LM interpretability designed to address these questions. Building on this framework, I will then discuss the more practical problem of building a concrete evaluation standard for the field. I will present insights from our recent work on building a mechanistic interpretability benchmark, which enables systematic comparison of different approaches to understanding model mechanisms. I will conclude by discussing an underexplored open problem: the presence of non-human-like concepts in language models. I will briefly discuss how this complicates our ability to understand and validate mechanistic explanations, and discuss ongoing work focused on understanding when and why non-human-like concepts arise in LMs—and what we could do with them.</p>
      <p><strong>Bio:</strong> Aaron Mueller is an assistant professor in the Department of Computer Science at Boston University. His research centers on developing language modeling methods and evaluations inspired by causal and linguistic principles, and applying these for precise model control and improved efficiency. He completed a Ph.D. at Johns Hopkins University and was a Zuckerman postdoctoral fellow at Northeastern and the Technion. His work has been published in ML and NLP venues (such as ICLR, ACL, and EMNLP) and has won awards at TMLR and ACL. He has recently been featured in the New York Times (2023) and IEEE Spectrum (2024) as an organizer of the BabyLM Challenge.</p>
    </details>
  </div>
</div>
<p><br /></p>

<div style="display: flex; flex-wrap: wrap;">

  <!-- Schedule section -->
  <div style="width: 70%; padding-right: 20px;">
    <h5> Schedule </h5>
    <div style="border: 1px solid #ddd; padding: 10px; border-radius: 5px; background-color: #f9f9f9;">
      <ul style="list-style-type: none; padding: 0;">
        <li style="padding: 5px 0;"><strong>12:30 - 13:15</strong> &mdash; Lunch</li>
        <li style="padding: 5px 0;"><strong>13:15 - 14:05</strong> &mdash; Yuval Pinter <em>(Ben Gurion University of the Negev)</em></li>
        <li style="padding: 5px 0;"><strong>14:05 - 14:55</strong> &mdash; Tuhin Chakrabarty <em>(Stony Brook University)</em></li>
        <li style="padding: 5px 0;"><strong>14:55 - 15:15</strong> &mdash; Break</li>
        <li style="padding: 5px 0;"><strong>15:15 - 16:05</strong> &mdash; Sarah Wiegreffe <em>(Allen Institute for AI)</em></li>
        <li style="padding: 5px 0;"><strong>16:05 - 16:55</strong> &mdash; Yossi Levi <em>(Technion)</em></li>
        <li style="padding: 5px 0;"><strong>16:45 - 17:40</strong> &mdash; Aaron Mueller <em>(Boston University)</em></li>
      </ul>
    </div>
  </div>

  <!-- Organizers section -->
  <div style="width: 30%;">
    <h5> Organizers  </h5>
    <div style="display: flex; flex-direction: column; align-items: center;">
      <div style="text-align: center; margin-top: 30px;">
        <img src="/assets/img/frontiers-in-nlp/vered-shwartz.png" alt="Vered Shwartz" style="width: 150px; height: auto; " />
        <p><br />Vered Shwartz<br />UBC &amp; Vector Institute</p>
      </div>
    </div>
  </div>

</div>

<p>For questions, issues and inquiries, please email vshwartz@cs.ubc.ca</p>

			    </div>
		</div>
	</div>
</div>
<footer style="margin-top:50px">
  <div class="row z-depth-4" style="margin-bottom: 0px; background-color: #97D4E9;">
  <div class="col l8 m10 s12 left-align offset-l2 offset-m1">
      <div class="col l6 m8 s12 left-align">
        <div style="margin:20px">
            <div><b>Room X460</b></div>
            <div><b>Department of Computer Science</b></div>
            <div>201-2366 Main Mall</div>
            <div>Vancouver, BC Canada</div>
            <div>V6T 1Z4</div>
          <div>E-mail: <a href="mailto:ubc-nlp@cs.ubc.ca">ubc-nlp@cs.ubc.ca</a></div>
        </div>
      </div>
      <div class="col l6 m4 s12 right-align">
      <div style="margin: 20px;">
          <b>Find us on</b>
          <div>
              <a href="https://twitter.com/UBC_NLP"><img src="/assets/img/icons/twitter-square.svg" style="height:40px; padding-top:5px; padding-left:5px;"></a>
              <a href="https://github.com/NLP-UBC"><img src="/assets/img/icons/github-square.svg" style="height:40px; padding-top:5px; padding-left:5px;"></a>
          </div>
      </div>
    </div>
  </div>
</div>
<div class="row z-depth-4" style="margin-bottom: 0px; background-color: #002145;">
  <div class="col l4 m4 s12 center-align"> 
      <img src="/assets/img/ubc_dark.png" style="width:100%; min-width:200px; max-width: 300px; padding-top:15px; padding-left:15px; float: left;">
  </div>

  <div class="col l8 m8 s12 center-align"> 
    <div class="col l5 m6 s12" style="color: #EDEDEF; padding-top:18px; padding-right: 5px;">The UBC NLP group is partially sponsored by grants and awards from:
    </div>
    <div class="col l7 m6 s12 center-align">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
        <img src="/assets/img/sponsors/ai2_logo.png" style="height:60px; background-color: #FFFFFF; padding: 10px; margin-top:30px; margin-bottom:30px; border-radius: 10px">
    
    
    
        <img src="/assets/img/sponsors/cihr_logo.png" style="height:60px; background-color: #FFFFFF; padding: 10px; margin-top:30px; margin-bottom:30px; border-radius: 10px">
    
    
    
        <img src="/assets/img/sponsors/huawei_logo.png" style="height:60px; background-color: #FFFFFF; padding: 10px; margin-top:30px; margin-bottom:30px; border-radius: 10px">
    
    
    
        <img src="/assets/img/sponsors/mitacs_logo.jpeg" style="height:60px; background-color: #FFFFFF; padding: 10px; margin-top:30px; margin-bottom:30px; border-radius: 10px">
    
    
    
        <img src="/assets/img/sponsors/nserc_logo.png" style="height:60px; background-color: #FFFFFF; padding: 10px; margin-top:30px; margin-bottom:30px; border-radius: 10px">
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    </div>
  </div>
</div>
</footer>




    </div>
  </body>
</html>
